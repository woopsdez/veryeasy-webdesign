{"version":3,"sources":["../src/no-doubled-conjunction.js"],"names":[],"mappings":";AACA;;;;;;;;;;;;;;;;;;kBAce,UAAU,OAAV,EAAiC;AAAA,QAAd,OAAc,yDAAJ,EAAI;;AAC5C,QAAM,SAAS,mCAAe,OAAf,CAAf;AAD4C,QAErC,MAFqC,GAEG,OAFH,CAErC,MAFqC;AAAA,QAE7B,MAF6B,GAEG,OAFH,CAE7B,MAF6B;AAAA,QAErB,SAFqB,GAEG,OAFH,CAErB,SAFqB;AAAA,QAEV,SAFU,GAEG,OAFH,CAEV,SAFU;;AAG5C,+BACK,OAAO,SADZ,YACuB,IADvB,EAC4B;AACpB,YAAI,OAAO,WAAP,CAAmB,IAAnB,EAAyB,CAAC,OAAO,IAAR,EAAc,OAAO,KAArB,EAA4B,OAAO,UAAnC,EAA+C,OAAO,QAAtD,CAAzB,CAAJ,EAA+F;AAC3F;AACH;AACD,YAAM,SAAS,mCAAiB,IAAjB,CAAf;AACA,YAAM,OAAO,OAAO,QAAP,EAAb;AACA,YAAM,iBAAiB,SAAjB,cAAiB,CAAC,IAAD;AAAA,mBAAU,KAAK,IAAL,KAAc,yBAAe,QAAvC;AAAA,SAAvB;AACA,YAAI,YAAY,6BAAe,IAAf,EAAqB;AACjC,wBAAY;AADqB,SAArB,EAEb,MAFa,CAEN,cAFM,CAAhB;;;AAKA,YAAG,UAAU,MAAV,KAAqB,CAAxB,EAA2B;AACvB;AACH;AACD,eAAO,+BAAe,IAAf,CAAoB,qBAAa;AACtC,gBAAM,oBAAoB,SAApB,iBAAoB,CAAC,QAAD,EAAc;AACtC,oBAAI,SAAS,UAAU,mBAAV,CAA8B,SAAS,GAAvC,CAAb;AACA,oBAAI,oBAAoB,OAAO,MAAP,CAAc,UAAC,KAAD;AAAA,2BAAW,MAAM,GAAN,KAAc,KAAzB;AAAA,iBAAd,CAAxB;AACA,uBAAO,CAAC,QAAD,EAAW,iBAAX,CAAP;AACD,aAJD;AAKA,gBAAI,aAAa,IAAjB;AACA,sBAAU,GAAV,CAAc,iBAAd,EAAiC,MAAjC,CAAwC,UAAC,IAAD,EAAO,OAAP,EAAmB;AACzD,oBAAI,QAAQ,UAAZ;;AADyD,8CAExB,OAFwB;;AAAA,oBAEpD,QAFoD;AAAA,oBAE1C,cAF0C;;AAAA,2CAGtB,IAHsB;;AAAA,oBAGpD,aAHoD;AAAA,oBAGrC,WAHqC;;AAIzD,oBAAI,eAAe,YAAY,MAAZ,GAAqB,CAAxC,EAA2C;AACzC,4BAAQ,YAAY,CAAZ,CAAR;AACD;AACD,oBAAI,eAAe,MAAf,GAAwB,CAA5B,EAA+B;AAC7B,wBAAI,SAAS,eAAe,CAAf,EAAkB,YAAlB,KAAmC,MAAM,YAAtD,EAAoE;AAClE,4BAAI,gBAAgB,OAAO,yBAAP,CAAiC;AACnD,kCAAM,SAAS,GAAT,CAAa,KAAb,CAAmB,IAD0B;AAEnD,oCAAQ,SAAS,GAAT,CAAa,KAAb,CAAmB,MAAnB,IAA6B,eAAe,CAAf,EAAkB,aAAlB,GAAkC,CAA/D;AAF2C,yBAAjC,CAApB;;AAKA,4BAAI,UAAU;AACV,mCAAO;AADG,yBAAd;AAGA,+BAAO,IAAP,EAAa,IAAI,SAAJ,uBAAoC,OAApC,CAAb;AACD;AACF;AACD,6BAAa,KAAb;AACA,uBAAO,OAAP;AACD,aAtBD;AAuBD,SA9BM,CAAP;AA+BH,KA/CL;AAiDH,C;;AAjED;;AACA;;AACA;;AACA;;;;;;;;AA8DC","file":"no-doubled-conjunction.js","sourcesContent":["// LICENSE : MIT\n\"use strict\";\nimport {RuleHelper} from \"textlint-rule-helper\";\nimport {getTokenizer} from \"kuromojin\";\nimport {split as splitSentences, Syntax as SentenceSyntax} from \"sentence-splitter\";\nimport StringSource from \"textlint-util-to-string\";\n\n/*\n    1. Paragraph Node -> text\n    2. text -> sentences\n    3. tokenize sentence\n    4. report error if found word that match the rule.\n\n    TODO: need abstraction\n */\nexport default function (context, options = {}) {\n    const helper = new RuleHelper(context);\n    const {Syntax, report, getSource, RuleError} = context;\n    return {\n        [Syntax.Paragraph](node){\n            if (helper.isChildNode(node, [Syntax.Link, Syntax.Image, Syntax.BlockQuote, Syntax.Emphasis])) {\n                return;\n            }\n            const source = new StringSource(node);\n            const text = source.toString();\n            const isSentenceNode = (node) => node.type === SentenceSyntax.Sentence;\n            let sentences = splitSentences(text, {\n                charRegExp: /[。\\?\\!？！]/\n            }).filter(isSentenceNode);\n            // if not have a sentence, early return\n            // It is for avoiding error of emptyArray.reduce().\n            if(sentences.length === 0) {\n                return;\n            }\n            return getTokenizer().then(tokenizer => {\n              const selectConjenction = (sentence) => {\n                let tokens = tokenizer.tokenizeForSentence(sentence.raw);\n                let conjunctionTokens = tokens.filter((token) => token.pos === \"接続詞\");\n                return [sentence, conjunctionTokens];\n              };\n              let prev_token = null;\n              sentences.map(selectConjenction).reduce((prev, current) => {\n                let token = prev_token;\n                let [sentence, current_tokens] = current;\n                let [prev_sentence, prev_tokens] = prev;\n                if (prev_tokens && prev_tokens.length > 0) {\n                  token = prev_tokens[0];\n                }\n                if (current_tokens.length > 0) {\n                  if (token && current_tokens[0].surface_form === token.surface_form) {\n                    let originalIndex = source.originalIndexFromPosition({\n                      line: sentence.loc.start.line,\n                      column: sentence.loc.start.column + (current_tokens[0].word_position - 1)\n                    });\n                    // padding position\n                    var padding = {\n                        index: originalIndex\n                    };\n                    report(node, new RuleError(`同じ接続詞が連続して使われています。`, padding));\n                  }\n                }\n                prev_token = token;\n                return current;\n              });\n            });\n        }\n    }\n};\n"]}